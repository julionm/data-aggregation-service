Aggregation service

The MapReduce framework is a good option to aggregate ad click events. The directed acyclic graph (DAG) is a good model for it [9]. The key to the DAG model is to break down the system into small computing units, like the Map/Aggregate/Reduce nodes, as shown in Figure 5.
Image represents two distinct data aggregation service architectures. The top architecture, labeled 'Aggregate every minute,' depicts a simple pipeline. Data Input feeds into a 'Map' component, which then sends its output to two separate 'Aggregate' components. Each 'Aggregate' component outputs an 'Ad count.' The entire process is enclosed within a dashed box labeled 'Aggregation Service.' The bottom architecture, labeled 'Top 100 aggregation,' shows a more complex MapReduce-like structure. Data Input flows into a 'Map' component, which then feeds into two 'Aggregate' components. These 'Aggregate' components, in turn, feed into a 'Reduce' component, which outputs 'Top 100 Ads.' Similar to the top architecture, this is also contained within a dashed box labeled 'Aggregation Service.' Both diagrams illustrate different approaches to aggregating data, with the top focusing on simple per-minute counts and the bottom aiming to identify the top 100 ads.
Figure 5 Aggregation service

Each node is responsible for one single task and it sends the processing result to its downstream nodes.

Map node

A Map node reads data from a data source, and then filters and transforms the data. For example, a Map node sends ads with ad_id % 2 = 0 to node 1, and the other ads go to node 2, as shown in Figure 6.
Image represents a simplified data processing pipeline. Data Input feeds into a central 'Map' node. The Map node then routes the data to two different 'Aggregate' nodes based on a condition: if the modulo 2 of the 'ad_id' field is 0, the data is sent to 'Aggregate Node 1'; otherwise (if the modulo 2 of 'ad_id' is 1), the data is sent to 'Aggregate Node 2'. The arrows indicate the direction of data flow, with labels specifying the routing condition ('ad_id %2 = 0' and 'ad_id %2 = 1'). This suggests a system that partitions data based on a specific field ('ad_id') and then performs separate aggregation operations on each partition.
Figure 6 Map operation

You might be wondering why we need the Map node. An alternative option is to set up Kafka partitions or tags and let the aggregate nodes subscribe to Kafka directly. This works, but the input data may need to be cleaned or normalized, and these operations can be done by the Map node. Another reason is that we may not have control over how data is produced and therefore events with the same ad_id might land in different Kafka partitions.

Aggregate node

An Aggregate node counts ad click events by ad_id in memory every minute. In the MapReduce paradigm, the Aggregate node is part of the Reduce. So the map-aggregate-reduce process really means map-reduce-reduce.

Reduce node

A Reduce node reduces aggregated results from all “Aggregate” nodes to the final result. For example, as shown in Figure 7, there are three aggregation nodes and each contains the top 3 most clicked ads within the node. The Reduce node reduces the total number of most clicked ads to 3.
Image represents a data processing pipeline for finding the top 3 most clicked ads in the last minute. The pipeline begins with an 'Aggregate' stage showing three boxes, each containing ad IDs and their click counts. The first box contains 'ad1: 12', 'ad3: 5', 'ad2: 3'; the second box contains 'ad7: 9', 'ad10: 4', 'ad8: 3'; and the third box contains 'ad13: 8', 'ad11: 4', 'ad15: 3'. Arrows connect these boxes to a 'Reduce' stage, which combines the data. The 'Reduce' stage shows a single box containing 'ad1: 12', 'ad7: 9', 'ad13: 8', representing the aggregation of the top click counts. Finally, an arrow points from the 'Reduce' stage to an 'Output' stage, which displays the text: 'Top 3 most clicked ads from the last minute are ad1, ad7 and ad13'. The task is explicitly stated as 'Task: get top 3' to the left of the Aggregate stage.
Figure 7 Reduce node

The DAG model represents the well-known MapReduce paradigm. It is designed to take big data and use parallel distributed computing to turn big data into little- or regular-sized data.

In the DAG model, intermediate data can be stored in memory and different nodes communicate with each other through either TCP (nodes running in different processes) or shared memory (nodes running in different threads).
Main use cases

Now that we understand how MapReduce works at the high level, let’s take a look at how it can be utilized to support the main use cases:

    Aggregate the number of clicks of adid in the last _M mins.

    Return top N most clicked adids in the last _M minutes.

    Data filtering.

Use case 1: aggregate the number of clicks

As shown in Figure 8, input events are partitioned by ad_id (ad_id % 3) in Map nodes and are then aggregated by Aggregation nodes.
Image represents a data processing pipeline for clickstream analysis. A rectangular box labeled 'All Events' on the left represents the input stream of all ad clicks. This stream is fed into a 'Map' stage, depicted as a larger rectangle, which separates events based on `ad_id`. A legend shows diamonds representing `ad_id = 3`, circles for `ad_id = 1`, and squares for `ad_id = 2`. The Map stage sorts these events into three separate streams, each containing only one `ad_id`. These streams then flow into an 'Aggregate' stage, another larger rectangle, where events are grouped by `ad_id` and counted. The output, labeled 'Outputs,' shows the aggregated results: 'ad3 was clicked 4 times in the past 1 minute,' 'ad1 was clicked 3 times in the past 1 minute,' and 'ad2 was clicked 5 times in the past 1 minute,' indicating a one-minute aggregation interval. The entire process is depicted as a flow from left to right, with dashed lines separating the 'Inputs,' 'Map,' 'Aggregate,' and 'Outputs' stages.
Figure 8 Aggregate the number of clicks
Use case 2: return top N most clicked ads

Figure 9 shows a simplified design of getting the top 3 most clicked ads, which can be extended to top N. Input events are mapped using ad_id and each Aggregate node maintains a heap data structure to get the top 3 ads within the node efficiently. In the last step, the Reduce node reduces 9 ads (top 3 from each aggregate node) to the top 3 most clicked ads every minute.
Image represents a data processing pipeline for determining the top 3 most clicked ads within a one-minute interval. The pipeline begins with 'Inputs,' specifically a box labeled 'Top 3 most clicked ads (Minute interval aggregation)' representing the desired output. 'All Events,' a central box, feeds into three separate 'Map' stages. Each 'Map' stage receives a subset of events: one receives 'Events: ad3, ad6, ad9, ad12, ad15'; another receives 'Events: ad1, ad4, ad7, ad10, ad13'; and the last receives 'Events: ad2, ad5, ad8, ad11, ad14.' These subsets are then processed through individual 'Aggregate' stages, each counting the occurrences of each ad ID. The 'Aggregate' stages output lists like 'ad3: 12, ad6: 5, ad9: 3, ad12: 1, ad15: 1,' showing ad IDs and their counts. These aggregated counts are then fed into a single 'Reduce' stage, which combines the counts from all ads across the three 'Aggregate' stages. The 'Reduce' stage outputs a consolidated list like 'ad3: 12, ad1: 9, ad2: 8, ad6: 5, ad4: 4, ad5: 4, ad9: 3, ad7: 3, ad8: 3,' showing all ads and their total counts. Finally, the 'Outputs' section displays the result: 'Top 3 most clicked ads in the past 1 minute are ad3, ad1 and ad2,' derived from the top three counts in the 'Reduce' stage's output. The entire process illustrates a MapReduce-like approach to real-time ad click aggregation.
Figure 9 Return top N most clicked ads
Use case 3: data filtering

To support data filtering like “show me the aggregated click count for ad001 within the USA only”, we can pre-define filtering criteria and aggregate based on them. For example, the aggregation results look like this for ad001 and ad002:
ad_id	click_minute	country	count
ad001	202101010001	USA	100
ad001	202101010001	GPB	200
ad001	202101010001	others	3000
ad002	202101010001	USA	10
ad002	202101010001	GPB	25
ad002	202101010001	others	12

Table 16 Aggregation results (filter by country)

This technique is called the star schema [11], which is widely used in data warehouses. The filtering fields are called dimensions. This approach has the following benefits:

    It is simple to understand and build.

    The current aggregation service can be reused to create more dimensions in the star schema. No additional component is needed.

    Accessing data based on filtering criteria is fast because the result is pre-calculated.

A limitation with this approach is that it creates many more buckets and records, especially when we have a lot of filtering criteria.